# -*- coding: utf-8 -*-
"""BOT-TRAINING

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H-cAR59-MxvqG7-Hw089gWPEdx1wQrv_
"""

# üöÄ AI Model Training with Dataset Download (Google Colab)
import os
import zipfile
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt

# ‚úÖ Mount Google Drive for storing dataset & model
from google.colab import drive
drive.mount('/content/drive')

# ‚úÖ Set up directories in Google Drive
BASE_PATH = "/content/drive/My Drive/e-waste-ai/"
DATASET_PATH = os.path.join(BASE_PATH, "dataset")

if not os.path.exists(BASE_PATH):
    os.makedirs(BASE_PATH)

if not os.path.exists(DATASET_PATH):
    os.makedirs(DATASET_PATH)

# ‚úÖ Step 1: Download Dataset from Your Google Drive Link
from google.colab import auth
from googleapiclient.discovery import build
import gdown

auth.authenticate_user()
drive_service = build('drive', 'v3')

# Google Drive folder ID (Extracted from your link)
FOLDER_ID = "1PeyuWT-0FJ0-x00poow8MaXXQPEpnTbj"

# List all files in the folder
results = drive_service.files().list(q=f"'{FOLDER_ID}' in parents",
                                     fields="files(id, name)").execute()
files = results.get('files', [])

if not files:
    print("‚ùå No files found in the dataset folder.")
else:
    for file in files:
        file_id = file['id']
        file_name = file['name']
        output_path = os.path.join(BASE_PATH, file_name)

        if not os.path.exists(output_path):
            print(f"üì• Downloading {file_name}...")
            gdown.download(f"https://drive.google.com/uc?id={file_id}", output_path, quiet=False)

# ‚úÖ Step 2: Extract Dataset ZIP File
for file in os.listdir(BASE_PATH):
    if file.endswith(".zip"):
        zip_path = os.path.join(BASE_PATH, file)
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(DATASET_PATH)
        print(f"‚úÖ Extracted {file}")

print("‚úÖ Dataset is ready!")

# ‚úÖ Step 3: Define Image Processing & Augmentation
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 30  # Adjust based on dataset size

data_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # 20% validation split
)

train_data = data_gen.flow_from_directory(
    DATASET_PATH,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

val_data = data_gen.flow_from_directory(
    DATASET_PATH,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

class_labels = list(train_data.class_indices.keys())

# ‚úÖ Step 4: Define CNN Model
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(len(class_labels), activation='softmax')
])

# ‚úÖ Step 5: Compile Model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# ‚úÖ Step 6: Set Callbacks (Early Stopping, Best Model Saving)
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint(os.path.join(BASE_PATH, "best_ewaste_classifier.h5"), save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)
]

# ‚úÖ Step 7: Train the Model
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=EPOCHS,
    callbacks=callbacks
)

# ‚úÖ Step 8: Save Final Model
model.save(os.path.join(BASE_PATH, "final_ewaste_classifier.h5"))
print("‚úÖ Model saved in Google Drive!")

# ‚úÖ Step 9: Plot Training History
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()